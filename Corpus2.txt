The past few years have seen a veritable explosion of randomized experiments in development economics. At the fall 2008 NEUDC conference, a large conference in development economics attended mainly by young researchers and PhD students, 24 papers reported on randomized field experiments, out of the 112 papers that used microeconomics data (laboratory experiments excluded). This is up from 4 in 2004. At the falll 2008 BREAD conference, the premier conference on development economics, 4 of the 8 papers invited were randomized field experiments. Three out of the 6 papers using microeconomic data from developing countries published in 2008 or forthcoming in the Quarterly Journal of Economics involve randomized assignment. The enthusiasm is not limited to academia. At the World Bank, there are 67 ongoing randomized evaluations (out of 89 ongoing program evaluations) in the African region alone. Perhaps inevitably, this progress has also generated a rising tide of criticism. Almost all of the criticism is well meant, recognizing the benefits of such experiments while suggesting that we not forget that there are a lot of important questions that randomized experiments cannot answer. Much of it is also not new. Indeed, most of the standard objections (and some not so standard ones) may be found in a single seminal piece written by James Heckman more than a decade ago (Heckman 1992). Much of this criticism has been useful—even when we do not entirely agree with it—both in helping us define the strengths and limitations of randomized experiments, and in clarifying where the field needs to go next. However, we argue that much of this criticism misses (or at least insufficiently emphasizes) the main reasons why there has been so much excitement surrounding experimental research in development economics. We then return to the various criticisms, in part to clarify and qualify them and to argue that, because of an imperfect recognition of what is exciting about the experimental agenda, there is a tendency to set up false oppositions between experimental work and other forms of research. Experimental research in development economics, like earlier research in labor economics, health, and education, started from a concern about the reliable identification of program effects in the face of complex and multiple channels of causality. In general, participants to a program differ from nonparticipants in many ways, and we have no information on how they would have fared had they not been participating. This makes it difficult to separate the “causal effect” of the program (i.e., for a given participant, the difference between the outcome he experienced under the program and the outcome he would have experienced if he had not been participating) from other factors. A central problem is selection, the fact that participants may be systematically different from nonparticipants. Although the treatment effect for each person cannot be identified, experiments make it possible to vary one factor at a time and therefore provide internally valid estimates of the average treatment effect for a population of interest. [For detailed discussions of the evaluation problem, see Heckman & Vytlacil (2008a) and Imbens & Woolridge (2008).]

The experimental work in the mid-1990s (e.g., Glewwe et al. 2004, 2009; Banerjee et al. 2005) was aimed at answering very basic questions about the educational production function: Does better access to inputs (textbooks, flipcharts in classes, lower student-teacher ratios) matter for school outcomes (attendance, test scores), and if so, by how much?

The motivating theoretical framework was thus very simple, but this research produced a number of surprising results, both negative and positive: For example, improving access to textbooks from one per four or more students to one per every two does not affect average test scores (Glewwe et al. 2009); halving the teacher-student ratio also had no effect (Banerjee et al. 2005). However, a study of treatment for intestinal worms in schools in Kenya (Miguel & Kremer 2004) showed that a deworming treatment that costs 49 cents per child per year can reduce absenteeism by 25%. In part, this is because of externalities: Worms are transmitted when children walk barefoot in places where other children who are infected by worms have defecated. As a result, in terms of increasing attendance, deworming is nearly 20 times as effective as hiring an extra teacher (the cost for an extra child-year of education was $3.25 with deworming, whereas the cost was approximately $60 for the extra teacher program, despite the fact the extra teacher was paid only $25 or so a month), even though both “work” in the sense of generating statistically significant improvements.

What this research was making clear is that, at the level of the efficacy of individual ingredients of the educational production function, our intuition (or economic theory per se) was unlikely to be very helpful—how could we possibly know, a priori, that deworming is so much more effective than hiring a teacher. More generally, a bulletin of the Abdul Latif Jameel Poverty Action Lab (2005) compares the cost per extra child-year of education induced across an array of different strategies. The costs vary widely, between $3.50 per extra child-year for deworming and $6000 per extra child-year for the primary education component of PROGRESA, the Mexican Conditional Cash Transfer Program. Some of these programs (such as the PROGRESA programs) may have other objectives as well, but for those whose main goal is to increase education, it is clear that some are much cheaper than others. Even excluding PROGRESA, the cost per extra year of education induced ranges from $3.25 to more than $200. Thus, even when comparing across programs to achieve the same goal, the rates of returns of public investment are far from being equalized.

Moreover, it became clear that economists were not the only people who were clueless; implementing organizations were not much better informed. For example, the nongovernmental organization (NGO) that financed the deworming intervention was also initially enthusiastic about giving children school uniforms, even though a randomized evaluation showed that the cost of giving children a free uniform worked out to be $100 per extra child-year of schooling.

Several important conclusions emerged from this experience. First, effective policy-making requires making judgments about the efficacy of individual program components—without much guidance from a priori knowledge. Second, however, it is also difficult to learn about these individual components from observational (i.e., nonexperimental) data. The reason is that observational data on the educational production function often comes from school systems that have adopted a given model, which consists of more than one input. The variation in school inputs we observe therefore comes from attempts to change the model, which, for good reasons, involves making multiple changes at the same time. Although there are exceptions, this means that a lot of the policy-relevant knowledge that requires observing the effects of variation in individual components of a package may not be available in observational data. This provides a first motivation for experiments.

One of the immediate implications of this observation is that, given the fixed cost of organizing an experiment and the fact that experiments necessarily require some time when program implementation has to be slowed down (to make use of the results), it is worth doing multiple experiments at the same time on the same population to evaluate alternative potential variants of the program. For example, the World Bank provided money to school committees to hire extra teachers on short contracts to reduce grade-1 class sizes in Kenya. When researchers worked with the school system to set up an evaluation of the program, they did not just assign the entire program to the randomly selected treatment schools (Duflo et al. 2008a). Instead, they introduced two additional dimensions of variation: (a) training of the school committee that received the money to monitor the extra teacher and (b) tracking by prior achievement. Using this design, researchers can estimate the impact of class-size reduction without changing pedagogy; the relative merit of young, extra teachers on short contracts versus regular, experienced, civil servant teachers; the role that suitably empowered school committees can play; and the impact of tracking by achievement in primary school. As in Banerjee et al. (2005), albeit in a different context, the study did not find that reducing class size without any other changes has a significant impact. However, it showed a strong positive impact of switching from the regular teacher to a contract teacher, a positive and significant impact of class-size reduction when coupled with school committee empowerment, and, for a given class size, strong benefit of tracking students both for the weaker and the stronger students.

Other “multiple treatment experiments” include Banerjee et al. (2007) (remedial education and computer-assisted learning), Duflo et al. (2006) and Dupas (2007) (various HIV-AIDS prevention strategies among adolescents), Banerjee et al. (2009) (information and mobilization experiments in primary schools in India), Banerjee et al. (2008) (demand and supply factors in improving immunization rates in India), and Gine et al. (2008) (two strategies to help smokers quit smoking).

A related observation is that, from the point of view of building a useable knowledge base, there is a need for a process of dynamic learning because experimental results are often surprising and therefore require further clarification. Duflo et al. (2008c,d) reflects exactly such an iterative process, where a succession of experiments on fertilizer use was run over a period of several years. Each result prompted the need to try out a series of new variations to better understand the results of the previous one.

In addition, from the point of view of optimal learning, it is often worth testing a broad intervention first to see whether there is an overall effect and then, if it is found to work, delving into its individual components to understand what part of the broad program works.1 Policy experiments often stop at the first step. One example is the popular PROGRESA-Opportunidades program in Mexico, which combined a cash transfer to women in poor families that was conditional on “good behavior” (e.g., investments in education and preventive health) and some upgrading of education and health facilities. The program has been replicated in many countries, often along with a randomized evaluation (Fizbein & Schady 2009). However, only an ongoing study in Morocco formed and compared different treatment groups so that researchers could evaluate the importance of the much-praised conditionalities. In this experiment, one group of villages receives a purely unconditional transfer, one group receives a weak conditionality transfer (e.g., attendance requirements are verified only by teachers), and two groups receive stricter variants of the conditionality (in one group, children's attendance is supervised by inspectors; in the other, it is verified daily with a fingerprint recognition device).

Although all this seems obvious in retrospect, it was only after the first few experiments that both researchers and the implementing organizations fully appreciated the significance of such a design. From the point of view of the organizations, it became clear that there was value in setting up relatively long-term relationships with researchers, so that the experimentation could constitute a process of ongoing learning and multiple experiments of mutual interests could be designed. In other words, there was less emphasis on one-off evaluations, where the researcher is brought in to evaluate a specific program that the organization has already decided to evaluate. This is a difference with the evaluation literature in the United States or Canada where, with a few important exceptions (e.g., Angrist et al. 2009), the programs to be evaluated are mainly chosen by the implementing agencies and the researchers are evaluators only.

From the point of view of the researchers, this design offered the possibility of moving from the role of the evaluator to the role of a coexperimenter, which included an important role in defining what gets evaluated. In other words, the researcher was now being offered the option of defining the question to be answered, thus drawing upon his knowledge of what else was known and the received theory. For example, when Seva Mandir, an NGO in Rajasthan, India, with whom we have had a long-standing relationship, was interested in improving the quality of their informal schools, their initial idea was to implement a teacher incentive program based on test scores. However, they were persuaded by the results from Glewwe et al. (2003) that showed that teacher incentives could result in teaching to the test or other short-run manipulations of test scores. They then decided to implement an incentive program based on teacher presence. To measure attendance in very sparsely populated areas where schools are difficult to access, Duflo and Hanna (Duflo et al. 2007) proposed the use of cameras with date and time stamps. Although Seva Mandir was initially surprised by the suggestion, they agreed to try it out. In program schools (the “camera schools”), teachers took a picture of themselves and their students twice a day (morning and afternoon), and their salary was computed as a (nonlinear) function of the number of days they attended. The results were striking (Duflo et al. 2007): Teacher absence dropped from 40% to 20% while students’ performance improved.

Seva Mandir was convinced by these results and decided to continue the program. However, they did not give up on the hope of improving the teachers’ intrinsic motivation. Instead of extending the camera program in all of their schools immediately, they decided to continue it in the schools where it had already been introduced and spend some time experimenting with other programs, both in schools with cameras and in schools without. With Sendhil Mullainathan, they brainstormed about ways to motivate teachers. One idea was to give every child a diary to write in every day based on work done in school. On days when the student or the teacher was absent, the diary was to remain blank or the date was to be crossed out. Parents were supposed to look at the diary every week. The hope was that they would register the extent of teacher and child absence. This approach, it turned out, did not succeed: Parents started with such a low opinion of school that the diary tended to persuade them that something was happening, regardless of number of absences. Indeed, parents of diary schools had a higher opinion than did those of nondiary schools, and there was no impact on teacher presence. However, the diaries were popular with both students and teachers, and their use induced teachers to work harder. Test scores improved in the diary schools. It thus appears that the diaries failed as a tool to improve teacher presence but succeeded as a pedagogical tool. However, because this was not a hypothesis put forward in the initial experimental design, it may just be a statistical accident. Thus, Seva Mandir will now put cameras in all schools (after several years, they continue to have a large impact on presence and tests scores), while they also conduct a new diary experiment to see if the results on pedagogy persist.

One important consequence of this process has been the growing realization in the research community that the most important element of the experimental approach may lie in the power (when working with a friendly implementing partner) to vary individual elements of the treatment in a way that helps us answer conceptual questions (albeit policy relevant ones) that could never be reliably answered in any other way.2 One telling example is Berry (2008). While incentives based on school participation and performance have become very popular, it is not clear whether the incentives should target children [as in the programs evaluated in Angrist et al. (2008) and Angrist & Lavy (2009)] or parents (as in Kremer et al. 2007). If the family were fully efficient, the choice of the target should not make a difference, but otherwise it might. To answer this question, Berry designed a program in the slums of Delhi where students (or their parents) were provided incentives (in the form of toys or money) based on the child's improvement in reading. He found that, for initially weak students, rewarding the child is more effective than rewarding the parents in terms of improving test scores, whereas the opposite is true for initially strong students. The ability to vary who receives the incentives within the same context and in the same experiment is what made this study possible.

Experiments are thus emerging as a powerful tool for testing theories. Although the theories to be tested are different, the motivation of the recent literature in experimental development economics is similar to that of the first generation of experiments in the United States, which were designed to identify well-designed parameters (i.e., income and substitution effect in the negative income tax experiments, moral hazard in the Rand health insurance experiment, etc.). Interventions are being designed and evaluated not only to show the average treatment effect for a particular policy of program, but also to allow identification of specific economic parameters. One example is a project conducted by Karlan & Zinman (2005) in collaboration with a South African lender that gives small loans to high-risk borrowers at high interest rates. The experiment was designed to test the relative weights of ex post repayment burden (including moral hazard) and ex ante adverse selection in loan default. Potential borrowers with the same observable risk are randomly offered a high or a low interest rate in an initial letter. Individuals then decide whether to borrow at the solicitation's offer rate. Of those who apply at the higher rate, half are randomly offered a new lower contract interest rate when they are actually given the loan, whereas the remaining half continue at the offer rate. Individuals did not know ex ante that the contract rate could differ from the offer rate. The researchers then compared repayment performance of the loans in all three groups. The comparison of those who responded to the high offer interest rate with those who responded to the low offer interest rate in the population that received the same low contract rate allows the identification of the adverse selection effect; comparing those who faced the same offer rate but differing contract rates identifies the repayment burden effect.

The study found that women exhibit adverse selection but men exhibit moral hazard. The fact that this difference was unexpected poses something of a problem for the paper (Is it a statistical fluke or a real phenomenon?) but its methodological contribution is undisputed. The basic idea of varying prices ex post and ex ante to identify different parameters has since been replicated in several different studies. Ashraf et al. (2007) and Cohen & Dupas (2007) exploit it to understand the relationship between the price paid for a health protection good and its utilization. Raising the price could affect usage through a screening effect (those who buy at a higher price care more) or a “psychological sunk cost effect.” To separate these effects, they randomize the offer price as well as the actual paid price. The effect the offer price has on keeping the actual price fixed identifies the screening effect, whereas the variation in the actual price (with a fixed offer price) pins down the sunk cost effect. Ashraf et al. (2007) studied this for a water-purification product, whereas Cohen & Dupas (2007) focused on bed nets. Neither study shows much evidence of a psychological sunk cost effect. The experimental variation was key here, and not only to avoid bias: In the world, we are unlikely to observe a large number of people who face different offer prices but the same actual price. These types of experiments are reminiscent of the motivation of the early social experiments (such as the negative income tax experiments) that aimed to obtain distinct wage and income variations to estimate income and substitution effects that were not available in observational data (Heckman 1992).
Other examples of this type of work are the experiments designed to assess whether there is a demand for commitment products, which could be demanded by self-aware people with self-control problems. Ashraf et al. (2006) worked with microfinance institutions in the Philippines to offer their clients a savings product that let them choose to commit not to withdraw the money before a specific time or amount goal was reached. Gine et al. (2008) worked with the same organization to invite smokers who wanted to quit to put a “contract” on themselves: Money in a special savings account would be forfeited if they failed a urine test for nicotine after several weeks. Both cases were designed by the economists to solve a real-world problem, but they also came with a strong theoretical motivation. The fact that these were new ideas that came from researchers made it natural to set up a randomized evaluation: Because the cases were experimental in nature, the partners were happy to try them out first with a subset of their clients/beneficiaries. These two sets of examples are focused on individual behavior. Experiments can also be set up to understand the way institutions function. An example is Bertrand et al. 2009, who set up an experiment to understand the structure of corruption in the process of obtaining a driving license in Delhi. They recruited people who are aiming to get a driving license and set up three groups, one that receives a bonus for obtaining a driving license quickly, one that gets free driving lessons, and a control group. They found that those in the “bonus” group get their licenses faster, but those who get the free driving lessons do not. They also found that those in the bonus group are more likely to pay an agent to get the license (who, they conjecture, bribes someone). They also found that the applicants who hired an agent were less likely to have taken a driving test before getting a driving license. Although they did not appear to find that those in the bonus group who get licenses are systematically less likely to know how to drive than those in the control group (which would be the litmus test that corruption does result in an inefficient allocation of driving licenses), this experiment provides suggestive evidence that corruption in this case does more than “grease the wheels” of the system.
Digital technology is the representation of information in bits. This reduces the cost of storage, computation, and transmission of data. Research on digital economics examines whether and how digital technology changes economic activity. Understanding the effects of digital technology does not require fundamentally new economic theory. However, it requires a different emphasis. Studying digital economics starts with the question of “what is different?” What is easier to do when information is represented by bits rather than atoms? Digital technology often means that costs may constrain economic actions. Therefore, digital economics explores how standard economic models change as certain costs fall substantially and perhaps approach zero. We emphasize how this shift in costs can be divided into five types: (i) Lower search costs (ii) Lower replication costs (iii) Lower transportation costs (iv) Lower tracking costs (v) Lower verification costs. Search costs are lower in digital environments, enlarging the potential scope and quality of search. Digital goods can be replicated at zero cost, meaning they are often non-rival. The role of geographic distance changes as the cost of transportation for digital goods and information is approximately zero. Digital technologies make it easy to track any one individual’s behavior. Last, digital verification can make it easier to certify the reputation and trustworthiness of any one individual, firm, or organization in the digital economy. Each of these cost changes draws on a different set of well-established economic models, primarily search, non-rival goods, transportation cost, price discrimination, and reputation models. Early research tested straightforward models of lower costs. For example, the search literature of the late 1990s and early 2000s built directly on earlier models by Diamond (1971) and Varian (1980). As we detail below, empirical work emerged that found some inconsistencies with the simple models, and so richer models and empirical analysis of the cost reductions developed to take account of the subtleties of the digital context. Other authors have also emphasized the role of lower costs for digital economics (e.g., Shapiro and Varian 1998; Borenstein and Saloner 2001; and Smith, Bailey, and Brynjolfsson 2001). Ellison and Ellison (2005) discuss the implications of these lower search and transportation costs for industrial organization with respect to increasing returns, distance, and two-sided markets. Since their article, the digital economics literature has grown to contribute to the economics of crime, the economics of public goods, organizational economics, finance, urban economics, labor economics, development economics, health economics, political economy, media economics, public finance, and international economics. In this sense, we view digital economics as a way of thinking that touches many fields of economics. In addition to applying across many fields, these shifts in costs have transformed many aspects of the economy. After providing a brief history of digital technology and the Internet, we discuss each of the cost changes associated with digitization. In each section, we emphasize the key research questions that have driven the area and how they have evolved, and relate them to policy where applicable. We begin with a discussion of the effect of lower search costs, defined as the costs of looking for information. . Lower search costs affect prices and price dispersion. They affect product variety and media availability. They change matches in a variety of settings, from labor markets to dating. They have led to an increase in the prevalence of platform-based businesses, and affected the organization of some firms. We next turn to zero replication costs, which also affect pricing decisions including the decision to provide a good for free. This has enabled an increase in the provision of public goods such as Wikipedia, raising a number of new questions about the motivations for providing such goods. Zero replication costs create challenges with respect to excludability. Copyright can enforce excludability by using the law to overcome the non-rival nature of the technology. Consequently, copyright has become increasingly important to a variety of businesses and a core policy challenge related to digitization. Because the cost of transporting information stored in bits is near zero, this has changed the role of place-based constraints on economic activity, whether due to costs of physical transportation or policy. Digitization changes the ways governments can control the flow of information, from advertising restrictions to media blackouts. We then turn to examine a more recent literature that has identified two other cost changes: Tracking and verification costs. Tracking costs are the costs associated with connecting an individual person or firm with information about them. Low tracking costs enable novel forms of price discrimination as well as new ways to targeting advertising and other information. At the same time, better tracking has made privacy a key issue, generating a great deal of research and policy discussion. We conclude the discussion of cost changes by detailing changes in verification costs. The rise of online reputation systems has facilitated trust and created new markets. At the same time, such systems are imperfect, and can serve as platforms for fraud or discrimination. We finish by discussing the consequences of digitization for countries, regions, firms, and individuals. Digitization has affected productivity, trade, the economic role of cities, domestic and international outsourcing, consumer surplus, and how people spend their leisure time. 
Few recent concepts have attracted so much political, popular, and academic attention as that of "sustainable development." Although politicians are adept at embracing high-sounding objectives-especially when they are so loosely defined as to be consistent with almost any form of action (or inaction)-it is significant that "sustainable development" now figures as a goal in dozens of national environmental policy statements and in the opening paragraphs of Agenda 21, the massive shopping list of world actions adopted at the Earth Summit in Rio de Janeiro in June 1992. Agenda 21 states: In order to meet the challenges of environment and development, States decided to establish a new global partnership. This partnership commits all States to engage in a continuous and constructive dialogue, inspired by the need to achieve a more efficient and equitable world economy, keeping in view the increasing interdependence of the community of nations, and that sustainable development should become a priority item on the agenda of the international community (1). Whatever it means, therefore, sustainable development has become a highprofile objective. But to what is it that the world's politicians have committed themselves? While it is a popular pastime to collect different and incompatible definitions of sustainable development, inspection of the words and of their origins suggests that defining sustainable development is really not difficult. The difficult issue is determining what has to be done to achieve sustainable development, assuming it is a desirable goal. Slightly expanded, what is being referred to is sustainable economic development. The term "sustainable" is not open to much dispute: It means "enduring" and "lasting." So, sustainable development is development that lasts. Economic development could be defined narrowly in traditional terms as real gross national product (GNP) per capita, or real consumption per capita. Alternatively, it could be defined more broadly to include other indicators of development, such as education, health, and the "quality of life," including human freedom. One broad measure of economic development, the United Nations Development Programme's "Human Development Index" (HDI) (2), combines measures of subgoals--education, life expectancy, and gross domestic product (GDP)-to provide an index of a country's achievement relative to that of other countries. For current purposes, the definition of "development" does not matter much, although it is a legitimate source of investigation not least because "development" is a value word that invites any number of interpretations, which could proliferate to the point where "sustainable development" becomes meaningless. The economist would generally prefer to substitute per capita "utility" or "well-being" as the appropriate focus for sustainability (3-5). This at least provides a starting point to an inquiry into the determinants of sustainable development, and it is consistent with, though not identical to, the most widely used definition in the World Commission on Environment and Development's (WCED, 6) report in 1987 on North-South relations and the global environmental problem. There it is declared that: Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs (6, pA3). Substituting a development indicator for "needs" makes the equivalence of nondeclining well-being and the WCED definition fairly complete. The context of sustainable development has always been that of intergenerational equity, but the length of any particular sustainability time horizon. is of course open to debate. It must be a few generations at least, but it will not be infinity. We might appeal to some "coefficient of concern" to set some pragmatic limit on how far into the future we should look, say, 100 years. Of course, if individuals now already integrate future concerns into current actions and choices, "sustainability" is of little concern, since it will automatically be taken care of. 
